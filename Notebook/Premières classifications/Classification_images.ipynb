{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Classification_images.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"rEfSujULiOkb"},"source":["<H1> Premières classification d'images </H1>\n","\n","Dans ce notebook nous nous intéressons à la classification d'images. Nous présentons comment se comportent des clssifiers traditionnels, puis abordons les premiers réseaux de neurones. Enfin nous montrons l'utilisation des modèles de type convolution.   \n","\n","Le principe pour la classification d'images est le même que précédemment, il y a une étape de pré-traitement (convertir l'image, normaliser, etc.), une étape d'apprentissage avec un jeu d'apprentissage et de test (et bien entendu de la cross validation) et enfin l'établissement du modèle final qui peut être sauvegardé pour être réutilisé par la suite. "]},{"cell_type":"markdown","metadata":{"id":"_VQSC8frX5Ao"},"source":["## Installation\n"]},{"cell_type":"markdown","metadata":{"id":"5zICeCUqYCfS"},"source":["\n","Avant de commencer, il est nécessaire de déjà posséder dans son environnement toutes les librairies utiles. Dans la seconde cellule nous importons toutes les librairies qui seront utiles à ce notebook. Il se peut que, lorsque vous lanciez l'éxecution de cette cellule, une soit absente. Dans ce cas il est nécessaire de l'installer. Pour cela dans la cellule suivante utiliser la commande :  \n","\n","*! pip install nom_librairie*  \n","\n","**Attention :** il est fortement conseillé lorsque l'une des librairies doit être installer de relancer le kernel de votre notebook.\n","\n","**Remarque :** même si toutes les librairies sont importées dès le début, les librairies utiles pour des fonctions présentées au cours de ce notebook sont ré-importées de manière à indiquer d'où elles viennent et ainsi faciliter la réutilisation de la fonction dans un autre projet.\n","\n","\n","**Dans ce notebook nous utilisons UMAP pour faire de la réduction de dimensions. Les librairies suivantes doivent forcément être installées**"]},{"cell_type":"code","metadata":{"id":"I0OpnDBHYHmv","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1638548844367,"user_tz":-60,"elapsed":28250,"user":{"displayName":"Clara Lansalot","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhwgIIBWMsOkWsfBXXXWn1eeErNc0V79wPXeZq_=s64","userId":"06623982133577907028"}},"outputId":"88478658-8bde-4a19-c241-981d8ffb996c"},"source":["# utiliser cette cellule pour installer les librairies manquantes\n","# pour cela il suffit de taper dans cette cellule : !pip install nom_librairie_manquante\n","# d'exécuter la cellule et de relancer la cellule suivante pour voir si tout se passe bien\n","# recommencer tant que toutes les librairies ne sont pas installées ...\n","\n","# sous Colab il faut déjà intégrer ces deux librairies\n","\n","!pip install umap-learn[plot]\n","!pip install holoviews\n","!pip install -U ipykernel\n","\n","# eventuellement ne pas oublier de relancer le kernel du notebook"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting umap-learn[plot]\n","  Downloading umap-learn-0.5.2.tar.gz (86 kB)\n","\u001b[K     |████████████████████████████████| 86 kB 1.6 MB/s \n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from umap-learn[plot]) (1.19.5)\n","Requirement already satisfied: scikit-learn>=0.22 in /usr/local/lib/python3.7/dist-packages (from umap-learn[plot]) (1.0.1)\n","Requirement already satisfied: scipy>=1.0 in /usr/local/lib/python3.7/dist-packages (from umap-learn[plot]) (1.4.1)\n","Requirement already satisfied: numba>=0.49 in /usr/local/lib/python3.7/dist-packages (from umap-learn[plot]) (0.51.2)\n","Collecting pynndescent>=0.5\n","  Downloading pynndescent-0.5.5.tar.gz (1.1 MB)\n","\u001b[K     |████████████████████████████████| 1.1 MB 12.9 MB/s \n","\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from umap-learn[plot]) (4.62.3)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from umap-learn[plot]) (1.1.5)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from umap-learn[plot]) (3.2.2)\n","Collecting datashader\n","  Downloading datashader-0.13.0-py2.py3-none-any.whl (15.8 MB)\n","\u001b[K     |████████████████████████████████| 15.8 MB 31.2 MB/s \n","\u001b[?25hRequirement already satisfied: bokeh in /usr/local/lib/python3.7/dist-packages (from umap-learn[plot]) (2.3.3)\n","Requirement already satisfied: holoviews in /usr/local/lib/python3.7/dist-packages (from umap-learn[plot]) (1.14.6)\n","Requirement already satisfied: colorcet in /usr/local/lib/python3.7/dist-packages (from umap-learn[plot]) (2.0.6)\n","Requirement already satisfied: seaborn in /usr/local/lib/python3.7/dist-packages (from umap-learn[plot]) (0.11.2)\n","Requirement already satisfied: scikit-image in /usr/local/lib/python3.7/dist-packages (from umap-learn[plot]) (0.18.3)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from numba>=0.49->umap-learn[plot]) (57.4.0)\n","Requirement already satisfied: llvmlite<0.35,>=0.34.0.dev0 in /usr/local/lib/python3.7/dist-packages (from numba>=0.49->umap-learn[plot]) (0.34.0)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from pynndescent>=0.5->umap-learn[plot]) (1.1.0)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.22->umap-learn[plot]) (3.0.0)\n","Requirement already satisfied: PyYAML>=3.10 in /usr/local/lib/python3.7/dist-packages (from bokeh->umap-learn[plot]) (3.13)\n","Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from bokeh->umap-learn[plot]) (2.8.2)\n","Requirement already satisfied: tornado>=5.1 in /usr/local/lib/python3.7/dist-packages (from bokeh->umap-learn[plot]) (5.1.1)\n","Requirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from bokeh->umap-learn[plot]) (3.10.0.2)\n","Requirement already satisfied: pillow>=7.1.0 in /usr/local/lib/python3.7/dist-packages (from bokeh->umap-learn[plot]) (7.1.2)\n","Requirement already satisfied: packaging>=16.8 in /usr/local/lib/python3.7/dist-packages (from bokeh->umap-learn[plot]) (21.3)\n","Requirement already satisfied: Jinja2>=2.9 in /usr/local/lib/python3.7/dist-packages (from bokeh->umap-learn[plot]) (2.11.3)\n","Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from Jinja2>=2.9->bokeh->umap-learn[plot]) (2.0.1)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=16.8->bokeh->umap-learn[plot]) (3.0.6)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->bokeh->umap-learn[plot]) (1.15.0)\n","Requirement already satisfied: param>=1.7.0 in /usr/local/lib/python3.7/dist-packages (from colorcet->umap-learn[plot]) (1.12.0)\n","Requirement already satisfied: pyct>=0.4.4 in /usr/local/lib/python3.7/dist-packages (from colorcet->umap-learn[plot]) (0.4.8)\n","Requirement already satisfied: xarray>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from datashader->umap-learn[plot]) (0.18.2)\n","Collecting datashape>=0.5.1\n","  Downloading datashape-0.5.2.tar.gz (76 kB)\n","\u001b[K     |████████████████████████████████| 76 kB 5.1 MB/s \n","\u001b[?25hRequirement already satisfied: dask[complete]>=0.18.0 in /usr/local/lib/python3.7/dist-packages (from datashader->umap-learn[plot]) (2.12.0)\n","Requirement already satisfied: cloudpickle>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from dask[complete]>=0.18.0->datashader->umap-learn[plot]) (1.3.0)\n","Collecting partd>=0.3.10\n","  Downloading partd-1.2.0-py3-none-any.whl (19 kB)\n","Requirement already satisfied: toolz>=0.7.3 in /usr/local/lib/python3.7/dist-packages (from dask[complete]>=0.18.0->datashader->umap-learn[plot]) (0.11.2)\n","Collecting fsspec>=0.6.0\n","  Downloading fsspec-2021.11.1-py3-none-any.whl (132 kB)\n","\u001b[K     |████████████████████████████████| 132 kB 49.6 MB/s \n","\u001b[?25hCollecting distributed>=2.0\n","  Downloading distributed-2021.11.2-py3-none-any.whl (802 kB)\n","\u001b[K     |████████████████████████████████| 802 kB 41.1 MB/s \n","\u001b[?25hCollecting multipledispatch>=0.4.7\n","  Downloading multipledispatch-0.6.0-py3-none-any.whl (11 kB)\n","Collecting cloudpickle>=0.2.1\n","  Downloading cloudpickle-2.0.0-py3-none-any.whl (25 kB)\n","Collecting distributed>=2.0\n","  Downloading distributed-2021.11.1-py3-none-any.whl (793 kB)\n","\u001b[K     |████████████████████████████████| 793 kB 43.6 MB/s \n","\u001b[?25h  Downloading distributed-2021.11.0-py3-none-any.whl (793 kB)\n","\u001b[K     |████████████████████████████████| 793 kB 47.8 MB/s \n","\u001b[?25h  Downloading distributed-2021.10.0-py3-none-any.whl (791 kB)\n","\u001b[K     |████████████████████████████████| 791 kB 53.3 MB/s \n","\u001b[?25hRequirement already satisfied: zict>=0.1.3 in /usr/local/lib/python3.7/dist-packages (from distributed>=2.0->dask[complete]>=0.18.0->datashader->umap-learn[plot]) (2.0.0)\n","Requirement already satisfied: psutil>=5.0 in /usr/local/lib/python3.7/dist-packages (from distributed>=2.0->dask[complete]>=0.18.0->datashader->umap-learn[plot]) (5.4.8)\n","Requirement already satisfied: msgpack>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from distributed>=2.0->dask[complete]>=0.18.0->datashader->umap-learn[plot]) (1.0.3)\n","  Downloading distributed-2021.9.1-py3-none-any.whl (786 kB)\n","\u001b[K     |████████████████████████████████| 786 kB 44.7 MB/s \n","\u001b[?25h  Downloading distributed-2021.9.0-py3-none-any.whl (779 kB)\n","\u001b[K     |████████████████████████████████| 779 kB 48.9 MB/s \n","\u001b[?25hRequirement already satisfied: tblib>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from distributed>=2.0->dask[complete]>=0.18.0->datashader->umap-learn[plot]) (1.7.0)\n","Requirement already satisfied: click>=6.6 in /usr/local/lib/python3.7/dist-packages (from distributed>=2.0->dask[complete]>=0.18.0->datashader->umap-learn[plot]) (7.1.2)\n","  Downloading distributed-2021.8.1-py3-none-any.whl (778 kB)\n","\u001b[K     |████████████████████████████████| 778 kB 44.5 MB/s \n","\u001b[?25h  Downloading distributed-2021.8.0-py3-none-any.whl (776 kB)\n","\u001b[K     |████████████████████████████████| 776 kB 57.9 MB/s \n","\u001b[?25h  Downloading distributed-2021.7.2-py3-none-any.whl (769 kB)\n","\u001b[K     |████████████████████████████████| 769 kB 49.9 MB/s \n","\u001b[?25h  Downloading distributed-2021.7.1-py3-none-any.whl (766 kB)\n","\u001b[K     |████████████████████████████████| 766 kB 43.1 MB/s \n","\u001b[?25h  Downloading distributed-2021.7.0-py3-none-any.whl (1.0 MB)\n","\u001b[K     |████████████████████████████████| 1.0 MB 44.6 MB/s \n","\u001b[?25h  Downloading distributed-2021.6.2-py3-none-any.whl (722 kB)\n","\u001b[K     |████████████████████████████████| 722 kB 48.2 MB/s \n","\u001b[?25h  Downloading distributed-2021.6.1-py3-none-any.whl (722 kB)\n","\u001b[K     |████████████████████████████████| 722 kB 46.4 MB/s \n","\u001b[?25h  Downloading distributed-2021.6.0-py3-none-any.whl (715 kB)\n","\u001b[K     |████████████████████████████████| 715 kB 34.5 MB/s \n","\u001b[?25h  Downloading distributed-2021.5.1-py3-none-any.whl (705 kB)\n","\u001b[K     |████████████████████████████████| 705 kB 42.3 MB/s \n","\u001b[?25h  Downloading distributed-2021.5.0-py3-none-any.whl (699 kB)\n","\u001b[K     |████████████████████████████████| 699 kB 50.6 MB/s \n","\u001b[?25h  Downloading distributed-2021.4.1-py3-none-any.whl (696 kB)\n","\u001b[K     |████████████████████████████████| 696 kB 46.0 MB/s \n","\u001b[?25h  Downloading distributed-2021.4.0-py3-none-any.whl (684 kB)\n","\u001b[K     |████████████████████████████████| 684 kB 47.8 MB/s \n","\u001b[?25h  Downloading distributed-2021.3.1-py3-none-any.whl (679 kB)\n","\u001b[K     |████████████████████████████████| 679 kB 38.7 MB/s \n","\u001b[?25h  Downloading distributed-2021.3.0-py3-none-any.whl (675 kB)\n","\u001b[K     |████████████████████████████████| 675 kB 59.8 MB/s \n","\u001b[?25h  Downloading distributed-2021.2.0-py3-none-any.whl (675 kB)\n","\u001b[K     |████████████████████████████████| 675 kB 45.3 MB/s \n","\u001b[?25hRequirement already satisfied: sortedcontainers!=2.0.0,!=2.0.1 in /usr/local/lib/python3.7/dist-packages (from distributed>=2.0->dask[complete]>=0.18.0->datashader->umap-learn[plot]) (2.4.0)\n","  Downloading distributed-2021.1.1-py3-none-any.whl (672 kB)\n","\u001b[K     |████████████████████████████████| 672 kB 50.4 MB/s \n","\u001b[?25h  Downloading distributed-2021.1.0-py3-none-any.whl (671 kB)\n","\u001b[K     |████████████████████████████████| 671 kB 57.1 MB/s \n","\u001b[?25h  Downloading distributed-2020.12.0-py3-none-any.whl (669 kB)\n","\u001b[K     |████████████████████████████████| 669 kB 51.8 MB/s \n","\u001b[?25h  Downloading distributed-2.30.1-py3-none-any.whl (656 kB)\n","\u001b[K     |████████████████████████████████| 656 kB 51.9 MB/s \n","\u001b[?25hRequirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->umap-learn[plot]) (2018.9)\n","Collecting locket\n","  Downloading locket-0.2.1-py2.py3-none-any.whl (4.1 kB)\n","Requirement already satisfied: heapdict in /usr/local/lib/python3.7/dist-packages (from zict>=0.1.3->distributed>=2.0->dask[complete]>=0.18.0->datashader->umap-learn[plot]) (1.0.1)\n","Requirement already satisfied: panel>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from holoviews->umap-learn[plot]) (0.12.1)\n","Requirement already satisfied: pyviz-comms>=0.7.4 in /usr/local/lib/python3.7/dist-packages (from holoviews->umap-learn[plot]) (2.1.0)\n","Requirement already satisfied: bleach in /usr/local/lib/python3.7/dist-packages (from panel>=0.8.0->holoviews->umap-learn[plot]) (4.1.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from panel>=0.8.0->holoviews->umap-learn[plot]) (2.23.0)\n","Requirement already satisfied: markdown in /usr/local/lib/python3.7/dist-packages (from panel>=0.8.0->holoviews->umap-learn[plot]) (3.3.6)\n","Requirement already satisfied: webencodings in /usr/local/lib/python3.7/dist-packages (from bleach->panel>=0.8.0->holoviews->umap-learn[plot]) (0.5.1)\n","Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown->panel>=0.8.0->holoviews->umap-learn[plot]) (4.8.2)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown->panel>=0.8.0->holoviews->umap-learn[plot]) (3.6.0)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->umap-learn[plot]) (1.3.2)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->umap-learn[plot]) (0.11.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->panel>=0.8.0->holoviews->umap-learn[plot]) (2021.10.8)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->panel>=0.8.0->holoviews->umap-learn[plot]) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->panel>=0.8.0->holoviews->umap-learn[plot]) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->panel>=0.8.0->holoviews->umap-learn[plot]) (1.24.3)\n","Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from scikit-image->umap-learn[plot]) (1.2.0)\n","Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image->umap-learn[plot]) (2.6.3)\n","Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.7/dist-packages (from scikit-image->umap-learn[plot]) (2021.11.2)\n","Requirement already satisfied: imageio>=2.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image->umap-learn[plot]) (2.4.1)\n","Building wheels for collected packages: umap-learn, pynndescent, datashape\n","  Building wheel for umap-learn (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for umap-learn: filename=umap_learn-0.5.2-py3-none-any.whl size=82709 sha256=0b48d7053280afe18513c1f16df496b706e99611dd2059f7bb2c601688953ccb\n","  Stored in directory: /root/.cache/pip/wheels/84/1b/c6/aaf68a748122632967cef4dffef68224eb16798b6793257d82\n","  Building wheel for pynndescent (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pynndescent: filename=pynndescent-0.5.5-py3-none-any.whl size=52603 sha256=1493e3329d4392e4b4765e1bb838834c97579dba1bd3b8eb9d9dd8c4f3d53f8f\n","  Stored in directory: /root/.cache/pip/wheels/af/e9/33/04db1436df0757c42fda8ea6796d7a8586e23c85fac355f476\n","  Building wheel for datashape (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for datashape: filename=datashape-0.5.2-py3-none-any.whl size=59438 sha256=9cd68c8e0ebd28c63bd67de3e3c6a9287f08ce9a45478e3a620b914373bb2ef0\n","  Stored in directory: /root/.cache/pip/wheels/b5/b7/80/333a5c3312ed4cd54f5d5b869868c14e0c6002cb5c7238b52d\n","Successfully built umap-learn pynndescent datashape\n","Installing collected packages: locket, cloudpickle, partd, multipledispatch, fsspec, distributed, pynndescent, datashape, umap-learn, datashader\n","  Attempting uninstall: cloudpickle\n","    Found existing installation: cloudpickle 1.3.0\n","    Uninstalling cloudpickle-1.3.0:\n","      Successfully uninstalled cloudpickle-1.3.0\n","  Attempting uninstall: distributed\n","    Found existing installation: distributed 1.25.3\n","    Uninstalling distributed-1.25.3:\n","      Successfully uninstalled distributed-1.25.3\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","gym 0.17.3 requires cloudpickle<1.7.0,>=1.2.0, but you have cloudpickle 2.0.0 which is incompatible.\u001b[0m\n","Successfully installed cloudpickle-2.0.0 datashader-0.13.0 datashape-0.5.2 distributed-2.30.1 fsspec-2021.11.1 locket-0.2.1 multipledispatch-0.6.0 partd-1.2.0 pynndescent-0.5.5 umap-learn-0.5.2\n","Requirement already satisfied: holoviews in /usr/local/lib/python3.7/dist-packages (1.14.6)\n","Requirement already satisfied: panel>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from holoviews) (0.12.1)\n","Requirement already satisfied: colorcet in /usr/local/lib/python3.7/dist-packages (from holoviews) (2.0.6)\n","Requirement already satisfied: param<2.0,>=1.9.3 in /usr/local/lib/python3.7/dist-packages (from holoviews) (1.12.0)\n","Requirement already satisfied: pyviz-comms>=0.7.4 in /usr/local/lib/python3.7/dist-packages (from holoviews) (2.1.0)\n","Requirement already satisfied: pandas>=0.20.0 in /usr/local/lib/python3.7/dist-packages (from holoviews) (1.1.5)\n","Requirement already satisfied: numpy>=1.0 in /usr/local/lib/python3.7/dist-packages (from holoviews) (1.19.5)\n","Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.20.0->holoviews) (2018.9)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.20.0->holoviews) (2.8.2)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from panel>=0.8.0->holoviews) (2.23.0)\n","Requirement already satisfied: pyct>=0.4.4 in /usr/local/lib/python3.7/dist-packages (from panel>=0.8.0->holoviews) (0.4.8)\n","Requirement already satisfied: markdown in /usr/local/lib/python3.7/dist-packages (from panel>=0.8.0->holoviews) (3.3.6)\n","Requirement already satisfied: bokeh<2.4.0,>=2.3.0 in /usr/local/lib/python3.7/dist-packages (from panel>=0.8.0->holoviews) (2.3.3)\n","Requirement already satisfied: bleach in /usr/local/lib/python3.7/dist-packages (from panel>=0.8.0->holoviews) (4.1.0)\n","Requirement already satisfied: tqdm>=4.48.0 in /usr/local/lib/python3.7/dist-packages (from panel>=0.8.0->holoviews) (4.62.3)\n","Requirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from bokeh<2.4.0,>=2.3.0->panel>=0.8.0->holoviews) (3.10.0.2)\n","Requirement already satisfied: packaging>=16.8 in /usr/local/lib/python3.7/dist-packages (from bokeh<2.4.0,>=2.3.0->panel>=0.8.0->holoviews) (21.3)\n","Requirement already satisfied: PyYAML>=3.10 in /usr/local/lib/python3.7/dist-packages (from bokeh<2.4.0,>=2.3.0->panel>=0.8.0->holoviews) (3.13)\n","Requirement already satisfied: Jinja2>=2.9 in /usr/local/lib/python3.7/dist-packages (from bokeh<2.4.0,>=2.3.0->panel>=0.8.0->holoviews) (2.11.3)\n","Requirement already satisfied: tornado>=5.1 in /usr/local/lib/python3.7/dist-packages (from bokeh<2.4.0,>=2.3.0->panel>=0.8.0->holoviews) (5.1.1)\n","Requirement already satisfied: pillow>=7.1.0 in /usr/local/lib/python3.7/dist-packages (from bokeh<2.4.0,>=2.3.0->panel>=0.8.0->holoviews) (7.1.2)\n","Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from Jinja2>=2.9->bokeh<2.4.0,>=2.3.0->panel>=0.8.0->holoviews) (2.0.1)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=16.8->bokeh<2.4.0,>=2.3.0->panel>=0.8.0->holoviews) (3.0.6)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas>=0.20.0->holoviews) (1.15.0)\n","Requirement already satisfied: webencodings in /usr/local/lib/python3.7/dist-packages (from bleach->panel>=0.8.0->holoviews) (0.5.1)\n","Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown->panel>=0.8.0->holoviews) (4.8.2)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown->panel>=0.8.0->holoviews) (3.6.0)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->panel>=0.8.0->holoviews) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->panel>=0.8.0->holoviews) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->panel>=0.8.0->holoviews) (2021.10.8)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->panel>=0.8.0->holoviews) (2.10)\n","Requirement already satisfied: ipykernel in /usr/local/lib/python3.7/dist-packages (4.10.1)\n","Collecting ipykernel\n","  Downloading ipykernel-6.6.0-py3-none-any.whl (126 kB)\n","\u001b[K     |████████████████████████████████| 126 kB 34.5 MB/s \n","\u001b[?25hRequirement already satisfied: matplotlib-inline<0.2.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from ipykernel) (0.1.3)\n","Requirement already satisfied: importlib-metadata<5 in /usr/local/lib/python3.7/dist-packages (from ipykernel) (4.8.2)\n","Requirement already satisfied: debugpy<2.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from ipykernel) (1.0.0)\n","Requirement already satisfied: tornado<7.0,>=4.2 in /usr/local/lib/python3.7/dist-packages (from ipykernel) (5.1.1)\n","Collecting ipython>=7.23.1\n","  Downloading ipython-7.30.1-py3-none-any.whl (791 kB)\n","\u001b[K     |████████████████████████████████| 791 kB 33.3 MB/s \n","\u001b[?25hRequirement already satisfied: traitlets<6.0,>=5.1.0 in /usr/local/lib/python3.7/dist-packages (from ipykernel) (5.1.1)\n","Requirement already satisfied: argcomplete>=1.12.3 in /usr/local/lib/python3.7/dist-packages (from ipykernel) (1.12.3)\n","Requirement already satisfied: jupyter-client<8.0 in /usr/local/lib/python3.7/dist-packages (from ipykernel) (5.3.5)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata<5->ipykernel) (3.6.0)\n","Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata<5->ipykernel) (3.10.0.2)\n","Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.7/dist-packages (from ipython>=7.23.1->ipykernel) (4.8.0)\n","Requirement already satisfied: backcall in /usr/local/lib/python3.7/dist-packages (from ipython>=7.23.1->ipykernel) (0.2.0)\n","Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython>=7.23.1->ipykernel) (0.7.5)\n","Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython>=7.23.1->ipykernel) (4.4.2)\n","Collecting prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0\n","  Downloading prompt_toolkit-3.0.23-py3-none-any.whl (374 kB)\n","\u001b[K     |████████████████████████████████| 374 kB 40.4 MB/s \n","\u001b[?25hRequirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.7/dist-packages (from ipython>=7.23.1->ipykernel) (0.18.1)\n","Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.7/dist-packages (from ipython>=7.23.1->ipykernel) (57.4.0)\n","Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython>=7.23.1->ipykernel) (2.6.1)\n","Requirement already satisfied: parso<0.9.0,>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from jedi>=0.16->ipython>=7.23.1->ipykernel) (0.8.2)\n","Requirement already satisfied: jupyter-core>=4.6.0 in /usr/local/lib/python3.7/dist-packages (from jupyter-client<8.0->ipykernel) (4.9.1)\n","Requirement already satisfied: pyzmq>=13 in /usr/local/lib/python3.7/dist-packages (from jupyter-client<8.0->ipykernel) (22.3.0)\n","Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from jupyter-client<8.0->ipykernel) (2.8.2)\n","Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect>4.3->ipython>=7.23.1->ipykernel) (0.7.0)\n","Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=7.23.1->ipykernel) (0.2.5)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->jupyter-client<8.0->ipykernel) (1.15.0)\n","Installing collected packages: prompt-toolkit, ipython, ipykernel\n","  Attempting uninstall: prompt-toolkit\n","    Found existing installation: prompt-toolkit 1.0.18\n","    Uninstalling prompt-toolkit-1.0.18:\n","      Successfully uninstalled prompt-toolkit-1.0.18\n","  Attempting uninstall: ipython\n","    Found existing installation: ipython 5.5.0\n","    Uninstalling ipython-5.5.0:\n","      Successfully uninstalled ipython-5.5.0\n","  Attempting uninstall: ipykernel\n","    Found existing installation: ipykernel 4.10.1\n","    Uninstalling ipykernel-4.10.1:\n","      Successfully uninstalled ipykernel-4.10.1\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","jupyter-console 5.2.0 requires prompt-toolkit<2.0.0,>=1.0.0, but you have prompt-toolkit 3.0.23 which is incompatible.\n","google-colab 1.0.0 requires ipykernel~=4.10, but you have ipykernel 6.6.0 which is incompatible.\n","google-colab 1.0.0 requires ipython~=5.5.0, but you have ipython 7.30.1 which is incompatible.\u001b[0m\n","Successfully installed ipykernel-6.6.0 ipython-7.30.1 prompt-toolkit-3.0.23\n"]},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["IPython","ipykernel","prompt_toolkit"]}}},"metadata":{}}]},{"cell_type":"code","metadata":{"id":"FZw2C6r8YJWW"},"source":["# Importation des différentes librairies utiles pour le notebook\n","\n","#Sickit learn met régulièrement à jour des versions et \n","#indique des futurs warnings. \n","#ces deux lignes permettent de ne pas les afficher.\n","import warnings\n","warnings.filterwarnings(\"ignore\", category=FutureWarning)\n","\n","# librairies générales\n","import pickle \n","import pandas as pd\n","from scipy.stats import randint\n","import numpy as np\n","import string\n","import time\n","import base64\n","import re\n","import sys\n","import copy\n","\n","\n","# librairie affichage\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from PIL import Image\n","import plotly.graph_objs as go\n","import plotly.offline as py\n","\n","from sklearn.metrics import confusion_matrix\n","from sklearn.manifold import TSNE\n","from sklearn.decomposition import PCA\n","from sklearn.model_selection import KFold\n","from sklearn.model_selection import cross_val_score\n","from sklearn.pipeline import Pipeline\n","from sklearn.naive_bayes import GaussianNB\n","from sklearn.metrics import accuracy_score\n","\n","# TensorFlow et keras\n","import tensorflow as tf\n","from keras import layers\n","from keras import models\n","from keras import optimizers\n","from keras.preprocessing.image import ImageDataGenerator\n","from keras.preprocessing.image import img_to_array, load_img\n","from keras.callbacks import ModelCheckpoint, EarlyStopping\n","import keras\n","from keras.models import Sequential\n","from keras.layers import Dense, Dropout, Flatten\n","from keras.layers import Conv2D, MaxPooling2D\n","from keras.preprocessing import image\n","from tqdm import tqdm\n","from keras.models import load_model\n","\n","# Umap \n","import umap.plot"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5n70qVaRWwII"},"source":["def plot_curves_confusion (history,confusion_matrix,class_names):\n","  plt.figure(1,figsize=(16,6))\n","  plt.gcf().subplots_adjust(left = 0.125, bottom = 0.2, right = 1,\n","                          top = 0.9, wspace = 0.25, hspace = 0)\n","\n","  # division de la fenêtre graphique en 1 ligne, 3 colonnes,\n","  # graphique en position 1 - loss fonction\n","\n","  plt.subplot(1,3,1)\n","  plt.plot(history.history['loss'])\n","  plt.plot(history.history['val_loss'])\n","  plt.title('model loss')\n","  plt.ylabel('loss')\n","  plt.xlabel('epoch')\n","  plt.legend(['Training loss', 'Validation loss'], loc='upper left')\n","  # graphique en position 2 - accuracy\n","  plt.subplot(1,3,2)\n","  plt.plot(history.history['accuracy'])\n","  plt.plot(history.history['val_accuracy'])\n","  plt.title('model accuracy')\n","  plt.ylabel('accuracy')\n","  plt.xlabel('epoch')\n","  plt.legend(['Training accuracy', 'Validation accuracy'], loc='upper left')\n","  \n","  # matrice de correlation\n","  plt.subplot(1,3,3)\n","  sns.heatmap(conf,annot=True,fmt=\"d\",cmap='Blues',xticklabels=class_names, yticklabels=class_names)# label=class_names)\n","  # labels, title and ticks\n","  plt.xlabel('Predicted', fontsize=12)\n","  #plt.set_label_position('top') \n","  #plt.set_ticklabels(class_names, fontsize = 8)\n","  #plt.tick_top()\n","  plt.title(\"Correlation matrix\")\n","  plt.ylabel('True', fontsize=12)\n","  #plt.set_ticklabels(class_names, fontsize = 8)\n","  plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0s2sF7a-am4I"},"source":["Pour pouvoir sauvegarder sur votre répertoire Google Drive, il est nécessaire de fournir une autorisation. Pour cela il suffit d'éxecuter la ligne suivante et de saisir le code donné par Google."]},{"cell_type":"code","metadata":{"id":"q1zULJ9daqP6"},"source":["# pour monter son drive Google Drive local\n","from google.colab import drive\n","drive.mount('/content/gdrive')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0qe4Xtohau6v"},"source":["Corriger éventuellement la ligne ci-dessous pour mettre le chemin vers un répertoire spécifique dans votre répertoire Google Drive : "]},{"cell_type":"code","metadata":{"id":"ahDl5RV9tVfp"},"source":["import sys\n","my_local_drive='/content/gdrive/My Drive/Colab Notebooks/ML_FDS'\n","# Ajout du path pour les librairies, fonctions et données\n","sys.path.append(my_local_drive)\n","# Se positionner sur le répertoire associé\n","%cd $my_local_drive\n","\n","%pwd"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ivB0jdU1urpY"},"source":["# Le jeu de données MNINST"]},{"cell_type":"markdown","metadata":{"id":"rB_WdGARuwCZ"},"source":["Dans ce notebook nous allons utiliser le jeu de données Fashion MNIST qui est très classique. Issus de Zalano, il contient 70 000 images en basse résolution (28 x 28 pixels) et en niveaux de gris réparties en 10 catégories correspondant à des parties de vêtements. La base de données est repartie :  60000 exemples d'apprentissage et 10000 exemples de test. Elle est disponible sous keras. \n","\n","**Remarque :** Fashion MNIST est un peu plus intéressant à analyser que MNIST qui contient des chiffres de 0 à 9 et qui se classe facilement.   \n","\n","Lecture du jeu de données : "]},{"cell_type":"code","metadata":{"id":"t5wpZl_N4onV"},"source":["fashion_mnist = tf.keras.datasets.fashion_mnist\n","\n","(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"d4joid29wHvy"},"source":["Quelques informations sur le jeu de données montrent qu'il y a bien 60000 images pour l'entraînement et que chaque image est une matrice de 28 par 28."]},{"cell_type":"code","metadata":{"id":"wqhE84U8llhe"},"source":["print(\"Nombre d'exemples du jeu d'apprentissage : \", train_images.shape[0])\n","print (\"Format du jeu d'apprentissage : \", train_images.shape)\n","print(\"Format des labels pour le jeu d'apprentissage' : \", train_labels.shape, '\\n')\n","print(\"Nombre d'exemples du jeu de test : \", test_images.shape[0])\n","print (\"Nombre de features du jeu de test : \", test_images.shape[1])\n","print(\"Format des labels pour le jeu de test : \", test_labels.shape, '\\n')\n","print (\"Exemple de labels\",train_labels[0], '\\n')\n","print (\"Distribution des labels dans le jeu d'apprentissage\")\n","sns.countplot(np.array(train_labels))\n","plt.title(\"Nombre d'éléments par classe\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5_K3npqfxoU7"},"source":["Comme les labels sont encodés de 0 à 9, nous créons une classe pour labeliser les images. "]},{"cell_type":"code","metadata":{"id":"27bWD7Coxoyr"},"source":["print (\"Création d'une classe pour pouvoir labéliser les images lors de l'affichage\")\n","class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n","               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Q36QdolCyk7f"},"source":["Affichage de plusieurs images : "]},{"cell_type":"code","metadata":{"id":"FwJsd-w7otYD"},"source":["#Visualisation d'images\n","plt.figure(figsize=(10,10))\n","columns = 25\n","for i in range(columns):\n","    plt.subplot(5,5,i+1)\n","    plt.xticks([])\n","    plt.yticks([])\n","    plt.grid(False)\n","    plt.imshow(train_images[i],cmap=plt.cm.binary)\n","    plt.xlabel(class_names[train_labels[i]])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NJZgTqxStLEb"},"source":["### Visualisation des données\n","\n","Il est intéressant lorsque l'on a un jeu de données de regarder aussi comment ces dernières se répartissent dans l'espace. Cela peut permettre de déterminer des outliers, de voir des regroupements qui indiquent que souvent ces données sont plus facilement prédictibles ou au contraire des points de différentes classes très proches qui seront difficilement prédictibles,  etc.  \n","\n","Etant donné que les données ont un nombre trop grand de dimensions pour être visualisées, une solution consiste à utiliser de la réduction de dimensions. Il existe différentes méthodes comme par exemple : \n","* PCA (*Principal Component Analysis* - *Analyse en Composantes Principales*) qui consiste à transformer des variables corrélées entre elles (dites « corrélées » en statistique) en nouvelles variables décorrélées les unes des autres, i.e. les composantes principales. Ces nouvelles variables sont nommées « composantes principales ».\n","* T-SNE (*t-distributed Stochastic Neighbor Embedding*), plus récente et très utilisée en visualisation, utilise  une interprétation probabiliste des proximités, i.e. distribution de probabilité, au lieu des matrices de variance et co-variance de PCA.\n","* UMAP (*Uniform Manifold Approximation and Projection*), créé en 2018, utilise une technique de réduction de dimension non linéaire.\n","\n","Nous montrons par la suite différentes visualisations obtenues avec ces méthodes. Le jeu de données étant très volumineux (60 000 pour le jeu d'entraînement), nous utiliserons uniquement 3000 images pour illustrer.  \n","\n","Préparation des données :   \n","\n","**Attention :**  une image correspond à une matrice, ce qui veut dire que nous manipulons pour train_images un tenseur de la forme (60000, 28, 28) où 60000 correspond au nombre d'images et 28, 28 à la matrice de pixels de l'image. Pour pouvoir utiliser les méthodes précédentes ou pour pouvoir utiliser un classifier traditionnel, il est nécessaire de transformer la matrice associée à chaque image en un vecteur. Pour cela nous utilisons reshape qui va donc nous donner (60000, 784)."]},{"cell_type":"code","metadata":{"id":"ZrM6G481OKBn"},"source":["# reshape pour pouvoir afficher \n","nb_images=3000\n","print (\"Sélection de \",nb_images,\" images de train_images à visualiser\")\n","print (\"Forme du jeu de données à visualiser\",train_images[:nb_images].shape)\n","train_images_reshaped = train_images[:nb_images].reshape((nb_images, 28 * 28))\n","print (\"Forme du jeu de données après reshape (transformation en vecteur)\",train_images_reshaped.shape)\n","\n","# Definition des données à afficher\n","X_plot = train_images_reshaped[:nb_images]\n","y_plot = train_labels[:nb_images]\n","\n","label_dictionnary = {\n","        0:'T-shirt/top',\n","        1:'Trouser',\n","        2:'Pullover',\n","        3:'Dress',\n","        4:'Coat',\n","        5:'Sandal',\n","        6:'Shirt',\n","        7:'Sneaker',\n","        8:'Bag',\n","        9:'Ankle Boot',\n","    }\n","\n","def true_label(x):\n","    return label_dictionnary[x]    \n","\n","\n","Target_names=[]\n","for i in range(0,nb_images):\n","  Target_names=np.append(Target_names,true_label(y_plot[i]))    "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"X5G5R2XG8v8t"},"source":["Utilisation de PCA :"]},{"cell_type":"code","metadata":{"id":"frkfKcX5NlQ8"},"source":["# Utilisation de PCA\n","pca = PCA(n_components=2)\n","pca.fit(X_plot)\n","X_pca = pca.transform(X_plot)\n","\n","trace0 = go.Scatter(\n","    x = X_pca[:,0],\n","    y = X_pca[:,1],\n","    mode = 'markers',\n","    text = Target_names,\n","    showlegend = False,\n","    marker = dict(\n","        size = 8,\n","        color = y_plot,\n","        colorscale ='Jet',\n","        showscale = False,\n","        line = dict(\n","            width = 2,\n","            color = 'rgb(255, 255, 255)'\n","        ),\n","        opacity = 0.8\n","    )\n",")\n","data = [trace0]\n","\n","layout = go.Layout(\n","    title= 'Principal Component Analysis (PCA)',\n","    hovermode= 'closest',\n","    xaxis= dict(\n","         title= 'First Principal Component',\n","        ticklen= 5,\n","        zeroline= False,\n","        gridwidth= 2,\n","    ),\n","    yaxis=dict(\n","        title= 'Second Principal Component',\n","        ticklen= 5,\n","        gridwidth= 2,\n","    ),\n","    showlegend= True\n",")\n","\n","\n","fig = dict(data=data, layout=layout)\n","py.iplot(fig, filename='styled-scatter')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kjcfIIeh8a9s"},"source":["Nous pouvons constater qu'avec PCA il n'est pas facile de voir les différentes classes.  \n","\n","Utilisation de TSNE : "]},{"cell_type":"code","metadata":{"id":"Y-jASsJlH3jz"},"source":["# Utilisation de TSNE\n","tsne = TSNE(n_components=2)\n","tsne_results = tsne.fit_transform(X_plot) \n","\n","traceTSNE = go.Scatter(\n","    x = tsne_results[:,0],\n","    y = tsne_results[:,1],\n","    text = Target_names,\n","    mode = 'markers',\n","    showlegend = True,\n","    marker = dict(\n","        size = 8,\n","        color = y_plot,\n","        colorscale ='Jet',\n","        showscale = False,\n","        line = dict(\n","            width = 2,\n","            color = 'rgb(255, 255, 255)'\n","        ),\n","        opacity = 0.8\n","    )\n",")\n","data = [traceTSNE]\n","\n","layout = dict(title = 'TSNE (T-Distributed Stochastic Neighbour Embedding)',\n","              hovermode= 'closest',\n","              yaxis = dict(zeroline = False),\n","              xaxis = dict(zeroline = False),\n","              showlegend= False,\n","\n","             )\n","\n","fig = dict(data=data, layout=layout)\n","py.iplot(fig, filename='styled-scatter')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"naxJqG9A9vNG"},"source":["Nous constatons que T-SNE fait apparaître de manière plus précise des classes. Ces dernières seront sans doute plus simple à prédire.  \n","\n","Utilisation de UMAP : "]},{"cell_type":"code","metadata":{"id":"B9b7ldo7--QM"},"source":["mapper = umap.UMAP().fit(X_plot)\n","\n","# utilisation d'un dataframe pour afficher les moints \n","hover_data = pd.DataFrame({'index':np.arange(nb_images),\n","                           'label':y_plot})\n","hover_data['item'] = hover_data.label.map(label_dictionnary)\n","\n","# affichage de la visualisation dans le notebook\n","umap.plot.output_notebook()\n","\n","# Utilisation de umap en interactif\n","p = umap.plot.interactive(mapper, labels=y_plot, \n","                          hover_data=hover_data, point_size=6)\n","umap.plot.show(p)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kAqlJJXw_h8S"},"source":["Comme pour T-SNE nous voyons des classes qui semblent facile à prédire alors que d'autres sont clairement difficiles car très imbriquées."]},{"cell_type":"markdown","metadata":{"id":"Q7dQRhVUyd6W"},"source":["### Pré-traitement des données\n","\n","Inspection d'une image en affichant les valeurs des pixels :"]},{"cell_type":"code","metadata":{"id":"IsHFb2XDmIG2"},"source":["numimage=10\n","plt.figure()\n","plt.imshow(train_images[numimage], cmap=plt.cm.binary)\n","plt.colorbar()\n","plt.grid(False)\n","#plt.xlabel(class_names[train_labels[numimage]])\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gTDFPHMxzc1h"},"source":["**Transformation des donnees**  \n","Nous pouvons constater que les valeurs de gris des pixels varient de 0 à 255 (droite de l'image). Il faut donc les \"normaliser\" entre 0 et 1. Ne pas oublier d'effectuer le même traitement pour le jeu de test."]},{"cell_type":"code","metadata":{"id":"igypuuL7mViR"},"source":["# Sauvegarde des données avant transformation\n","train_images_original=copy.deepcopy(train_images)\n","test_images_original=copy.deepcopy(test_images)\n","\n","train_images = train_images / 255.0\n","test_images = test_images / 255.0"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8V0cP1m90kA3"},"source":["Affichage pour vérifier que la transformation a bien été réalisée."]},{"cell_type":"code","metadata":{"id":"skKC9XEr0kwb"},"source":["numimage=10\n","plt.figure()\n","plt.imshow(train_images[numimage], cmap=plt.cm.binary)\n","plt.colorbar()\n","plt.grid(False)\n","#plt.xlabel(class_names[train_labels[numimage]])\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hUAN6cBwBOEk"},"source":["## Un modèle \"traditionnel\"\n"]},{"cell_type":"markdown","metadata":{"id":"FIjW_Ep9-5QQ"},"source":["Dans cette section, nous utilisons un classifier \"traditionnel\", i.e. ceux que nous avons l'habitude d'utiliser précédemment. En fait, il s'agit ici de créer un modèle baseline afin de pouvoir avoir une comparaison.   \n","\n","Pour rappel, comme l'image est une matrice, il est d'abord nécessaire de l'\"applatir\", i.e. de la transformer en vecteur. Par la suite comme il y a beaucoup de features, nous utiliserons une PCA pour réduire le nombre de dimensions. Enfin nous utilisons un simple modèle Naïve Bayes.   \n","\n","De manière à ne pas être biaisé par la répartition des données, il est important de faire une cross validation. "]},{"cell_type":"code","metadata":{"id":"Rv8ygqrtWmEp"},"source":["seed=7\n","nb_splits=10\n","k_fold = KFold(n_splits=nb_splits, shuffle=True, random_state=seed)\n","\n","pipe = Pipeline([('pca',PCA(n_components=100)),\n","                 ('clf',GaussianNB())])\n","\n","scoring = 'accuracy'\n","# transformation des données d'entrainement en vecteur\n","train_images_reshaped = train_images.reshape((train_images.shape[0], 28 * 28))\n","\n","print (\"Evaluation de \",pipe[\"clf\"], \" sur \", nb_splits, \" splits\")\n","score = cross_val_score(pipe, train_images_reshaped, train_labels, cv=k_fold, scoring=scoring)\n","\n","print('Les différentes accuracy pour les \",nb_splits, \" évaluations sont : \\n',\n","      score,'\\n')\n","print ('Accuracy moyenne : %0.3f'%(score.mean()), \n","       ' standard deviation %0.3f'%(score.std()))\n","\n","# prediction sur les données de test\n","# attention ne pas oublier de faire un pipe.fit pour créer le modèle final\n","pipe.fit(train_images_reshaped, train_labels)\n","\n","# transformation des données de test en vecteur au lieu de matrice\n","test_images_reshaped=test_images.reshape((test_images.shape[0], 28 * 28))\n","y_predicted = pipe.predict(test_images_reshaped)\n","\n","print(\"Accuracy sur le jeu de test\", accuracy_score(test_labels, y_predicted))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"j1jPg2c-AG6W"},"source":["Il est clair que Naïve Bayes n'est sans doute pas le meilleur classifieur pour ce jeu de données. Nous pourrions utiliser SVC, RandomForest, etc. et tester les différents hyperparamètres (en utilisant par exemple gridsearchCV)."]},{"cell_type":"markdown","metadata":{"id":"joFxIKZk0uJj"},"source":["## Un modèle simple de réseau de neurones"]},{"cell_type":"markdown","metadata":{"id":"bwtBITeR0ze1"},"source":["La construction du réseau consiste à tout d'abord spécifier les différentes couches. Ce premier modèle est séquentiel. Nous allons tout d'abord 'applatir' la matrice en ajoutant une couche *flatten* qui permet ainsi d'obtenir un vecteur de taille 784 (24x24). Par la suite nous ajoutons une couche contenant 128 neurones avec comme fonction d'activation *relu*. Enfin la dernière couche contient 10 neurones (les différentes parties de vêtements possible) et comme il s'agit de classification multi-classes, la fonction d'activation est *softmax*."]},{"cell_type":"code","metadata":{"id":"l6JOVjHr1OWp"},"source":["model1 = models.Sequential()\n","model1.add(layers.Flatten(input_shape=(28, 28)))\n","model1.add(layers.Dense(128, activation='relu'))\n","model1.add(layers.Dense(10, activation='softmax'))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xrLnc1UT3jSi"},"source":["Pour afficher le modèle créé : "]},{"cell_type":"code","metadata":{"id":"hnpNecXr13a4"},"source":["print(model1.summary())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vVOoo7blDQZa"},"source":["Le modèle a 101770 paramètres !"]},{"cell_type":"markdown","metadata":{"id":"xnm3W7QMtC5l"},"source":["**Choix de la fonction de perte** :  \n","Nous sommes dans le cas d'une classification multi-classes, nous pouvons utiliser la cross entropy. Sous Keras, la *sparse_categorical_crossentropy* permet de réaliser la cross entropy sans nécessité que les labels aient, au préalable, été transformés par un one-hot encoding. \n"]},{"cell_type":"code","metadata":{"id":"_h0plVTOnFVF"},"source":["model1.compile(optimizer='adam',\n","              loss='sparse_categorical_crossentropy',\n","              metrics=['accuracy'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tuYEVxR0nIq_"},"source":["epochs=8\n","batch_size=64\n","history=model1.fit(train_images, train_labels, validation_data=(test_images, test_labels), epochs=epochs, batch_size=batch_size) \n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"v_W28rqaIF-b"},"source":["Il suffit alors d'afficher les courbes associées en *accuracy* et *loss* pour mieux comprendre le comportement."]},{"cell_type":"code","metadata":{"id":"ihWzbaTXW82m"},"source":["#utilisation de predict et argmax car c'est un softmax\n","# la valeur retournée est un vecteur de probabilité, on retient la plus grande valeur pour la classe\n","predict_y=model1.predict(test_images) \n","y_pred=np.argmax(predict_y,axis=1)\n","\n","\n","print (\"\\nRappel du modèle testé\")\n","print (model1.summary(),'\\n')\n","\n","print(\"Accuracy sur le jeu de test\", accuracy_score(test_labels, y_pred))\n","\n","conf=confusion_matrix(test_labels,y_pred)\n","plot_curves_confusion (history,conf,class_names)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OjfVJW8GD5Hw"},"source":["Nous voyons que l'accuracy de ce modèle de réseau de neurones simple (Multi Layer Perceptron) est nettement meilleur que celle du Naïve Bayes précédent.   \n","Il est intéressant de regarder la matrice de confusion et les visualisations précédentes pour vérifier que les classes qui étaient difficiles à prédire ... se retrouve souvent mal classées dans la matrice.   \n","\n","Il est possible bien entendu d'essayer d'encore améliorer ce modèle.\n","\n","**Attention :** dans l'expérience précédente nous n'avons pas fait un k-fold donc, même si le batchsize va prendre un échantilon à chaque passage, il va travailler sur les mêmes données. Dans la cellule suivante, nous montrons la modification à apporter pour pouvoir effectuer une cross validation et donc bien tester les performances réelles du modèle. Bien entendu, l'apprentissage est plus long ! "]},{"cell_type":"code","metadata":{"id":"cb5CsY21Kzhf"},"source":["# Concaténation du jeu d'apprentissage et de test\n","inputs = np.concatenate((train_images, test_images), axis=0)\n","targets = np.concatenate((train_labels, test_labels), axis=0)\n","\n","nb_folds=2\n","epochs=8\n","batch_size=64\n","\n","kfold = KFold(n_splits=nb_folds, shuffle=True)\n","\n","# Cross-validation \n","fold_no = 1\n","acc_per_fold = []\n","loss_per_fold = []\n","for train, test in kfold.split(inputs, targets):\n","\n","  # Definition de l'architecture du modèle\n","  model1 = models.Sequential()\n","  model1.add(layers.Flatten(input_shape=(28, 28)))\n","  model1.add(layers.Dense(128, activation='relu'))\n","  model1.add(layers.Dense(10, activation='softmax'))\n","\n","  # Compilation du modèle \n","  model1.compile(optimizer='adam',\n","              loss='sparse_categorical_crossentropy',\n","              metrics=['accuracy'])\n","  \n","  print('\\nEntrainement pour le fold  ', fold_no, ' ')\n","\n","  # Fit data sur les données\n","  history = model1.fit(inputs[train], targets[train],\n","              batch_size=batch_size,\n","              epochs=epochs)\n","\n","  # Récupération des métriques (accuracy et loss)\n","  scores = model1.evaluate(inputs[test], targets[test], verbose=0)\n","  print(f'Score pour fold {fold_no} : {model1.metrics_names[0]} = {scores[0]} - {model1.metrics_names[1]} = {scores[1]*100}%')\n","  acc_per_fold.append(scores[1] * 100)\n","  loss_per_fold.append(scores[0])\n","\n","  # Augmentation du nombre de folds\n","  fold_no = fold_no + 1\n","\n","# Les scores \n","print('Scores par fold :\\n')\n","for i in range(0, len(acc_per_fold)):\n","  print(\" \\tFold \",i+1, \"Accuracy : \",acc_per_fold[i],\"% - Loss: \",loss_per_fold[i])\n","print('\\nScores moyens pour tous les folds:')\n","print(f' \\tAccuracy : {np.mean(acc_per_fold)} (+- {np.std(acc_per_fold)})')\n","print(f' \\tLoss : {np.mean(loss_per_fold)}')\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MJzo9f8GKmjT"},"source":["# Un modèle plus compliqué : modèle à convolution (CNN)"]},{"cell_type":"markdown","metadata":{"id":"ljfsxcO2Kqwz"},"source":["Dans cette section nous utilisons un modèle de CNN particulièrement bien adapté aux images."]},{"cell_type":"markdown","metadata":{"id":"8tB21Wq9LZGH"},"source":["Comme les images sont en niveau de gris, il manque une dimension (couleur RVB). Pour cela nous transformons les données de la manière suivante : "]},{"cell_type":"code","metadata":{"id":"ZlbIhMJ0LBLA"},"source":["train_images = train_images_original.reshape(60000, 28, 28, 1)\n","test_images = test_images_original.reshape(10000, 28, 28, 1)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jNB0X0uiRLWi"},"source":["Contrairement au précédent, nous n'applatissons pas les données mais considérons que les données d'entrées sont bien de la forme 28x28 pixels (*input_shape*). Nous passons par l'intermédiaire d'une couche de convolution 2D qui est suivie par une maxPooling. Enfin nous applatissons (*Flatten*) la sortie du maxPooling et ajoutons une couche dense. La dernière couche, comme précédemment contient 10 neurones et une fonction de sortie softmax pour la classification multi-classes."]},{"cell_type":"code","metadata":{"id":"nTtestswLBq9"},"source":["cnnmodel = tf.keras.Sequential()\n","# 1 couche de convolution, avec nombre de filtres progressif 32\n","cnnmodel.add(Conv2D(filters=32, kernel_size=(3,3), input_shape=(28, 28, 1), activation='relu'))\n","cnnmodel.add(MaxPooling2D(pool_size=(2, 2)))\n","# remise à plat\n","cnnmodel.add(Flatten())\n","# Couche dense classique ANN\n","cnnmodel.add(Dense(100, activation='relu'))\n","# Couche de sortie (classes de 0 à 9)\n","cnnmodel.add(Dense(10, activation='softmax'))\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"l9_QgPoRMGGh"},"source":["Architecture du modèle :"]},{"cell_type":"code","metadata":{"id":"oczmUwtNMHou"},"source":["print (cnnmodel.summary())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"M4AqTMmWQVxW"},"source":["Cette fois ci il y a 542230 paramètres à appendre !"]},{"cell_type":"code","metadata":{"id":"Zu0mwuLdMPi2"},"source":["cnnmodel.compile(optimizer='adam', loss='sparse_categorical_crossentropy',metrics=['accuracy'])\n","\n","epochs=4\n","batch_size=64\n","history=cnnmodel.fit(train_images, train_labels, validation_data=(test_images, test_labels), epochs=epochs, batch_size=batch_size) \n","                  "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kVJbHUTOMisC"},"source":["#utilisation de predict_classes car c'est un softmax\n","#y_pred=model1.predict_classes(test_images) \n","predict_y=cnnmodel.predict(test_images) \n","y_pred=np.argmax(predict_y,axis=1)\n","\n","print (\"\\nRappel du modèle testé\")\n","print (cnnmodel.summary(),'\\n')\n","\n","print(\"Accuracy sur le jeu de test\", accuracy_score(test_labels, y_pred))\n","\n","conf=confusion_matrix(test_labels,y_pred)\n","plot_curves_confusion (history,conf,class_names)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"E7lEdoXVVgdW"},"source":["Le modèle par CNN obtient d'encore meilleurs résultats. \n","\n","Nous analysons les performances à l'aide d'une cross validation.\n"]},{"cell_type":"code","metadata":{"id":"5skyDqPdR-m-"},"source":["# Concaténation du jeu d'apprentissage et de test\n","inputs = np.concatenate((train_images, test_images), axis=0)\n","targets = np.concatenate((train_labels, test_labels), axis=0)\n","\n","nb_folds=2\n","epochs=4\n","batch_size=64\n","\n","kfold = KFold(n_splits=nb_folds, shuffle=True)\n","\n","# Cross-validation \n","fold_no = 1\n","acc_per_fold = []\n","loss_per_fold = []\n","for train, test in kfold.split(inputs, targets):\n","\n","  # Definition de l'architecture du modèle\n","  cnnmodel = tf.keras.Sequential()\n","  cnnmodel.add(Conv2D(filters=32, kernel_size=(3,3), input_shape=(28, 28, 1), activation='relu'))\n","  cnnmodel.add(MaxPooling2D(pool_size=(2, 2)))\n","  cnnmodel.add(Flatten())\n","  cnnmodel.add(Dense(100, activation='relu'))\n","  cnnmodel.add(Dense(10, activation='softmax'))\n","  \n","\n","  # Compilation du modèle \n","  cnnmodel.compile(optimizer='adam',\n","              loss='sparse_categorical_crossentropy',\n","              metrics=['accuracy'])\n","  \n","  print('\\nEntrainement pour le fold  ', fold_no, ' ')\n","\n","  # Fit data sur les données\n","  history = cnnmodel.fit(inputs[train], targets[train],\n","              batch_size=batch_size,\n","              epochs=epochs)\n","\n","  # Récupération des métriques (accuracy et loss)\n","  scores = cnnmodel.evaluate(inputs[test], targets[test], verbose=0)\n","  print(f'Score pour fold {fold_no} : {model1.metrics_names[0]} = {scores[0]} - {model1.metrics_names[1]} = {scores[1]*100}%')\n","  acc_per_fold.append(scores[1] * 100)\n","  loss_per_fold.append(scores[0])\n","\n","  # Augmentation du nombre de folds\n","  fold_no = fold_no + 1\n","\n","# Les scores \n","print('Scores par fold :\\n')\n","for i in range(0, len(acc_per_fold)):\n","  print(\" \\tFold \",i+1, \"Accuracy : \",acc_per_fold[i],\"% - Loss: \",loss_per_fold[i])\n","print('\\nScores moyens pour tous les folds:')\n","print(f' \\tAccuracy : {np.mean(acc_per_fold)} (+- {np.std(acc_per_fold)})')\n","print(f' \\tLoss : {np.mean(loss_per_fold)}')\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DH01aRcvWy2i"},"source":["Nous voyons qu'en faisant de la cross validation notre score est meilleur que le modèle précédent. Il peut bien entendu être encore amélioré.  Sur Fashion Mnist vous pouvez via des CNN obtenir assez facilement plus de 90% d'accuracy avec cross validation. \n"]},{"cell_type":"markdown","metadata":{"id":"rWSIMeXdXKXE"},"source":["### Sauvegarde et utilisation du modèle"]},{"cell_type":"markdown","metadata":{"id":"av5K58TZXOvb"},"source":["Apprendre un modèle peut, comme nous l'avons vu, être très long ! Aussi une fois le modèle appris, il est important de le sauvegarder.  \n","\n","**Rappel :** les expérimentations menées sont là pour voir si l'architecture définie nous donne un bon modèle. Par contre à chaque étape nous avons un jeu d'apprentissage et nous évaluons avec le jeu de test. Une fois ces expérimentations réalisées (et qui montrent que le modèle est performant), **il faut relancer le modèle avec tout le jeu de données avant de le sauvegarder**.  \n","\n","De manière à ne pas générer un modèle trop gros sur le disque, nous ne considérons ici que les images d'entrainement (ici il aurait fallu concaténer *train_images* avec *test_images* et *train_labels* avec *test_labels*).   \n","\n","L'apprentissage du modèle se fait tout simplement via fit :"]},{"cell_type":"code","metadata":{"id":"ChfE5hhHlV-j"},"source":["cnnmodel.fit(train_images, train_labels, epochs=4, batch_size=32, verbose=0)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UJ7kRa0CZEsW"},"source":["La sauvegarde se fait alors par la commande *save* "]},{"cell_type":"code","metadata":{"id":"2uQCs0OhZNLG"},"source":["cnnmodel.save('final_modelFMnist.h5')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5-mQd5ZCZl6Z"},"source":["Pour charger un modèle appris : "]},{"cell_type":"code","metadata":{"id":"Dm539qSbZmcX"},"source":["model_loaded = load_model('final_modelFMnist.h5')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hTv8clKVaBRq"},"source":["Il est maintenant possible de faire des prédictions. Comme le modèle sauvegardé n'a pas utilisé les images du jeu de test, nous pouvons les utiliser. "]},{"cell_type":"code","metadata":{"id":"gZGJtYy_sC1B"},"source":["# prédiction avec le modèle chargé \n","nb_images=15\n","predict_y=model_loaded.predict(test_images[0:nb_images]) \n","y_pred=np.argmax(predict_y,axis=1)\n","y_pred.astype('int')\n","for i in range(0,len(y_pred)):\n","  print (\"Classe réelle  \", class_names[test_labels[i]], \" - classe prédite\", class_names[int(y_pred[i])] )\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rGZQNdepDb_S"},"source":["Il reste bien entendu de très nombreuses choses à voir comme prendre en compte la génération d'images pour enrichir les données, le transfer learning pour utiliser des modèles déjà appris, les sauvegardes en cours d'apprentissage si ce dernier est très long, savoir ce qui est reconnu dans l'image pour la classification (e.g. https://arxiv.org/abs/1610.02391), etc..   \n"]}]}